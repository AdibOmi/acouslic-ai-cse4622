{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4248d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NPZFrameDataset(Dataset):\n",
    "    def __init__(self, npz_dir, files, transform=None, binary=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            npz_dir (str): path to .npz files\n",
    "            files (list[str]): list of filenames\n",
    "            transform (callable): torchvision-style transform (on torch.Tensor CxHxW)\n",
    "            binary (bool): if True → merge class 2 into 1 (binary classification)\n",
    "        \"\"\"\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "        self.binary = binary\n",
    "\n",
    "        for f in files:\n",
    "            case = np.load(os.path.join(npz_dir, f))\n",
    "            images = case[\"image\"].astype(np.float32)   # (F,H,W)\n",
    "            labels = case[\"label\"].astype(np.int64)     # (F,)\n",
    "\n",
    "            # normalize to [0,1] at load time\n",
    "            images = images / 255.0\n",
    "\n",
    "            if self.binary:\n",
    "                labels[labels == 2] = 1   # merge suboptimal with optimal\n",
    "\n",
    "            # flatten into (frame, label) list\n",
    "            for img, lbl in zip(images, labels):\n",
    "                self.samples.append((img, lbl))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, lbl = self.samples[idx]\n",
    "\n",
    "        # (H,W) → (1,H,W)\n",
    "        img = torch.from_numpy(img).unsqueeze(0)  # torch.float32\n",
    "\n",
    "        # resize to (1,224,224)\n",
    "        img = F.interpolate(\n",
    "            img.unsqueeze(0),\n",
    "            size=(224, 224),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False\n",
    "        ).squeeze(0)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, torch.tensor(lbl).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be5ba727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "class FrameClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.backbone = models.resnet50(pretrained=True)\n",
    "        # change first conv → grayscale\n",
    "        self.backbone.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.backbone.fc = nn.Linear(self.backbone.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):   # (B,1,H,W)\n",
    "        return self.backbone(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16838528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\torch310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3\\envs\\torch310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as T\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ==== Dataset with Augmentation ====\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    # --- Geometric ---\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomVerticalFlip(p=0.2),\n",
    "    T.RandomRotation(degrees=15),\n",
    "    T.RandomResizedCrop(224, scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "\n",
    "    # --- Photometric ---\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    T.RandomApply([T.GaussianBlur(kernel_size=3)], p=0.3),\n",
    "    T.RandomApply([T.GaussianBlur(kernel_size=5)], p=0.1),\n",
    "    T.RandomAdjustSharpness(sharpness_factor=2, p=0.3),\n",
    "\n",
    "    # --- Occlusion / noise (tensor ops) ---\n",
    "    T.RandomErasing(p=0.25, scale=(0.02, 0.1), ratio=(0.3, 3.3), value=0),\n",
    "    T.RandomApply([T.Lambda(lambda x: (x + 0.05*torch.randn_like(x)).clamp(0, 1))], p=0.2),\n",
    "\n",
    "    # --- Final (no ToTensor here) ---\n",
    "    T.Normalize(mean=[0.5], std=[0.5])  # for 1-channel tensors in [0,1]\n",
    "])\n",
    "\n",
    "val_transform = T.Compose([\n",
    "    T.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = NPZFrameDataset(\"D:/dataset/npz_80_tiny\",  \n",
    "                                sorted(os.listdir(\"D:/dataset/npz_80_tiny\"))[:210],  \n",
    "                                transform=train_transform)\n",
    "val_dataset = NPZFrameDataset(\"D:/dataset/converted_classifier_npz_compact\", \n",
    "                            sorted(os.listdir(\"D:/dataset/converted_classifier_npz_compact\"))[210:255], \n",
    "                            transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = FrameClassifier(num_classes=2).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)  # weight decay added\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=2, factor=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e93699d",
   "metadata": {},
   "source": [
    "## Training LOOP (DONT RUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84b05c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3092\\3069142313.py:13: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if device == \"cuda\" else None\n",
      "Epoch 1:   0%|          | 0/4200 [00:00<?, ?it/s]C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3092\\3069142313.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1: 100%|██████████| 4200/4200 [02:34<00:00, 27.10it/s, loss=0.263] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Avg Train Loss = 0.3681\n",
      "Val Loss: 0.1217 | Val Acc: 95.77%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9905    0.9659    0.9780     36852\n",
      "           1     0.3253    0.6382    0.4309       948\n",
      "\n",
      "    accuracy                         0.9577     37800\n",
      "   macro avg     0.6579    0.8021    0.7045     37800\n",
      "weighted avg     0.9738    0.9577    0.9643     37800\n",
      "\n",
      "✅ Best model saved with val acc = 95.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/4200 [00:00<?, ?it/s]C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3092\\3069142313.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 2: 100%|██████████| 4200/4200 [02:38<00:00, 26.43it/s, loss=0.263]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Avg Train Loss = 0.2713\n",
      "Val Loss: 0.2322 | Val Acc: 90.43%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9979    0.9037    0.9485     36852\n",
      "           1     0.1983    0.9262    0.3267       948\n",
      "\n",
      "    accuracy                         0.9043     37800\n",
      "   macro avg     0.5981    0.9149    0.6376     37800\n",
      "weighted avg     0.9778    0.9043    0.9329     37800\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 0/4200 [00:00<?, ?it/s]C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3092\\3069142313.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 3: 100%|██████████| 4200/4200 [02:37<00:00, 26.64it/s, loss=0.0261] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Avg Train Loss = 0.2402\n",
      "Val Loss: 0.1945 | Val Acc: 92.91%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9971    0.9300    0.9624     36852\n",
      "           1     0.2474    0.8945    0.3877       948\n",
      "\n",
      "    accuracy                         0.9291     37800\n",
      "   macro avg     0.6223    0.9123    0.6750     37800\n",
      "weighted avg     0.9783    0.9291    0.9480     37800\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|          | 0/4200 [00:00<?, ?it/s]C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3092\\3069142313.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 4: 100%|██████████| 4200/4200 [02:37<00:00, 26.63it/s, loss=0.0933] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Avg Train Loss = 0.2256\n",
      "Val Loss: 0.0912 | Val Acc: 96.69%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9927    0.9732    0.9828     36852\n",
      "           1     0.4090    0.7205    0.5218       948\n",
      "\n",
      "    accuracy                         0.9669     37800\n",
      "   macro avg     0.7008    0.8468    0.7523     37800\n",
      "weighted avg     0.9780    0.9669    0.9713     37800\n",
      "\n",
      "✅ Best model saved with val acc = 96.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5:   0%|          | 0/4200 [00:00<?, ?it/s]C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3092\\3069142313.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 5: 100%|██████████| 4200/4200 [02:38<00:00, 26.49it/s, loss=0.196]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Avg Train Loss = 0.2135\n",
      "Val Loss: 0.0935 | Val Acc: 96.33%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9945    0.9677    0.9809     36852\n",
      "           1     0.3866    0.7911    0.5194       948\n",
      "\n",
      "    accuracy                         0.9633     37800\n",
      "   macro avg     0.6905    0.8794    0.7502     37800\n",
      "weighted avg     0.9792    0.9633    0.9693     37800\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6:   0%|          | 0/4200 [00:00<?, ?it/s]C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3092\\3069142313.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 6: 100%|██████████| 4200/4200 [02:36<00:00, 26.79it/s, loss=0.527]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Avg Train Loss = 0.1992\n",
      "Val Loss: 0.2084 | Val Acc: 91.03%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9987    0.9092    0.9518     36852\n",
      "           1     0.2127    0.9536    0.3478       948\n",
      "\n",
      "    accuracy                         0.9103     37800\n",
      "   macro avg     0.6057    0.9314    0.6498     37800\n",
      "weighted avg     0.9790    0.9103    0.9367     37800\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:   0%|          | 0/4200 [00:00<?, ?it/s]C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3092\\3069142313.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 7: 100%|██████████| 4200/4200 [02:38<00:00, 26.43it/s, loss=0.00913]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Avg Train Loss = 0.1942\n",
      "Val Loss: 0.1212 | Val Acc: 95.11%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9963    0.9534    0.9744     36852\n",
      "           1     0.3223    0.8618    0.4691       948\n",
      "\n",
      "    accuracy                         0.9511     37800\n",
      "   macro avg     0.6593    0.9076    0.7217     37800\n",
      "weighted avg     0.9794    0.9511    0.9617     37800\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8:   0%|          | 0/4200 [00:00<?, ?it/s]C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3092\\3069142313.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 8: 100%|██████████| 4200/4200 [02:38<00:00, 26.52it/s, loss=0.0106] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Avg Train Loss = 0.1602\n",
      "Val Loss: 0.0574 | Val Acc: 97.63%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9933    0.9824    0.9878     36852\n",
      "           1     0.5196    0.7416    0.6110       948\n",
      "\n",
      "    accuracy                         0.9763     37800\n",
      "   macro avg     0.7564    0.8620    0.7994     37800\n",
      "weighted avg     0.9814    0.9763    0.9783     37800\n",
      "\n",
      "✅ Best model saved with val acc = 97.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9:   0%|          | 0/4200 [00:00<?, ?it/s]C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3092\\3069142313.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 9: 100%|██████████| 4200/4200 [02:35<00:00, 27.08it/s, loss=0.00912]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Avg Train Loss = 0.1467\n",
      "Val Loss: 0.1353 | Val Acc: 95.28%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9980    0.9534    0.9752     36852\n",
      "           1     0.3385    0.9262    0.4958       948\n",
      "\n",
      "    accuracy                         0.9528     37800\n",
      "   macro avg     0.6682    0.9398    0.7355     37800\n",
      "weighted avg     0.9815    0.9528    0.9632     37800\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10:   0%|          | 0/4200 [00:00<?, ?it/s]C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3092\\3069142313.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 10: 100%|██████████| 4200/4200 [02:38<00:00, 26.58it/s, loss=0.00969] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Avg Train Loss = 0.1399\n",
      "Val Loss: 0.0820 | Val Acc: 96.72%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9952    0.9710    0.9830     36852\n",
      "           1     0.4211    0.8186    0.5561       948\n",
      "\n",
      "    accuracy                         0.9672     37800\n",
      "   macro avg     0.7081    0.8948    0.7695     37800\n",
      "weighted avg     0.9808    0.9672    0.9723     37800\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11:   0%|          | 0/4200 [00:00<?, ?it/s]C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3092\\3069142313.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 11: 100%|██████████| 4200/4200 [02:38<00:00, 26.55it/s, loss=0.0158]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Avg Train Loss = 0.1343\n",
      "Val Loss: 0.0876 | Val Acc: 96.59%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9969    0.9680    0.9823     36852\n",
      "           1     0.4154    0.8829    0.5650       948\n",
      "\n",
      "    accuracy                         0.9659     37800\n",
      "   macro avg     0.7061    0.9255    0.7736     37800\n",
      "weighted avg     0.9823    0.9659    0.9718     37800\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12:   0%|          | 0/4200 [00:00<?, ?it/s]C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3092\\3069142313.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 12: 100%|██████████| 4200/4200 [02:38<00:00, 26.58it/s, loss=0.0224]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Avg Train Loss = 0.1121\n",
      "Val Loss: 0.0689 | Val Acc: 97.13%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9959    0.9746    0.9851     36852\n",
      "           1     0.4609    0.8449    0.5964       948\n",
      "\n",
      "    accuracy                         0.9713     37800\n",
      "   macro avg     0.7284    0.9098    0.7908     37800\n",
      "weighted avg     0.9825    0.9713    0.9754     37800\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13:   0%|          | 0/4200 [00:00<?, ?it/s]C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3092\\3069142313.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 13: 100%|██████████| 4200/4200 [02:38<00:00, 26.44it/s, loss=0.0171]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Avg Train Loss = 0.1080\n",
      "Val Loss: 0.0910 | Val Acc: 96.50%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9969    0.9671    0.9818     36852\n",
      "           1     0.4084    0.8840    0.5587       948\n",
      "\n",
      "    accuracy                         0.9650     37800\n",
      "   macro avg     0.7027    0.9255    0.7702     37800\n",
      "weighted avg     0.9822    0.9650    0.9712     37800\n",
      "\n",
      "⏹️ Early stopping triggered.\n",
      "Training finished. Best validation accuracy = 97.63%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ---- Config ----\n",
    "best_acc = 0.0\n",
    "patience = 5\n",
    "no_improve = 0\n",
    "num_epochs = 30\n",
    "save_path = \"D:/acouslic-ai-cse4622/saved_weights/best_frame_classifier_tiny.pth\"\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler() if device == \"cuda\" else None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ----------------------\n",
    "    # Training\n",
    "    # ----------------------\n",
    "    model.train()\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for images, labels in loop:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if scaler:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}: Avg Train Loss = {avg_loss:.4f}\")\n",
    "\n",
    "    # ----------------------\n",
    "    # Validation\n",
    "    # ----------------------\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0.0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = outputs.argmax(1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = 100 * correct / total\n",
    "\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "    print(classification_report(all_labels, all_preds, digits=4))\n",
    "\n",
    "    # ----------------------\n",
    "    # Scheduler + Early Stopping\n",
    "    # ----------------------\n",
    "    scheduler.step(val_acc)\n",
    "\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        no_improve = 0\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"best_acc\": best_acc\n",
    "        }, save_path)\n",
    "        print(f\"✅ Best model saved with val acc = {best_acc:.2f}%\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(\"⏹️ Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "print(f\"Training finished. Best validation accuracy = {best_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cba5f779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrameClassifier(\n",
       "  (backbone): ResNet(\n",
       "    (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# reload model with same architecture\n",
    "model = FrameClassifier(num_classes=2).to(device)\n",
    "checkpoint = torch.load(\"D:/acouslic-ai-cse4622/saved_weights/best_frame_classifier_tiny.pth\", map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state\"])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5fd4a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def predict_case(npz_path, model, device=\"cpu\", batch_size=16):\n",
    "    data = np.load(npz_path, allow_pickle=True)\n",
    "    images = data[\"image\"].astype(np.float32)  # (T,H,W)\n",
    "    labels = data[\"label\"].astype(np.int64)    # ground-truth (optional)\n",
    "\n",
    "    # normalize [0,1]\n",
    "    images = (images - images.min()) / (images.max() - images.min() + 1e-8)\n",
    "\n",
    "    preds_all = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(images), batch_size):\n",
    "            batch = images[i:i+batch_size]\n",
    "            batch = torch.tensor(batch).unsqueeze(1).to(device)  # (B,1,H,W)\n",
    "\n",
    "            # resize to 224x224\n",
    "            batch = F.interpolate(batch, size=(224,224),\n",
    "                                  mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "            outputs = model(batch)\n",
    "            preds = outputs.argmax(1).cpu().numpy()\n",
    "            preds_all.extend(preds)\n",
    "\n",
    "    return np.array(preds_all), labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bab14f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9740    1.0000    0.9868     36818\n",
      "           1     0.0000    0.0000    0.0000       240\n",
      "           2     0.0000    0.0000    0.0000       742\n",
      "\n",
      "    accuracy                         0.9740     37800\n",
      "   macro avg     0.3247    0.3333    0.3289     37800\n",
      "weighted avg     0.9487    0.9740    0.9612     37800\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\torch310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\User\\anaconda3\\envs\\torch310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\User\\anaconda3\\envs\\torch310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "TEST_DIR = \"D:/dataset/converted_classifier_npz_compact\"  # full 840-frame cases\n",
    "test_files = sorted(os.listdir(TEST_DIR))[-45:]  # example: last 45 for test\n",
    "\n",
    "all_preds, all_labels = [], []\n",
    "for f in test_files:\n",
    "    if not f.endswith(\".npz\"): continue\n",
    "    preds, labels = predict_case(os.path.join(TEST_DIR,f), model, device=device)\n",
    "    all_preds.extend(preds)\n",
    "    all_labels.extend(labels)\n",
    "\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f1f92c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 9450/9450 [01:30<00:00, 104.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Frame-level):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Background     0.9933    0.9824    0.9878     36852\n",
      "    Positive     0.5196    0.7416    0.6110       948\n",
      "\n",
      "    accuracy                         0.9763     37800\n",
      "   macro avg     0.7564    0.8620    0.7994     37800\n",
      "weighted avg     0.9814    0.9763    0.9783     37800\n",
      "\n",
      "Confusion Matrix:\n",
      "[[36202   650]\n",
      " [  245   703]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# ---- Load best checkpoint ----\n",
    "checkpoint = torch.load(\"D:/acouslic-ai-cse4622/saved_weights/best_frame_classifier_tiny.pth\", map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state\"])\n",
    "model.eval()\n",
    "\n",
    "# ---- Collect predictions ----\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(val_loader, desc=\"Inference\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = outputs.argmax(1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# ---- Metrics ----\n",
    "print(\"Classification Report (Frame-level):\")\n",
    "print(classification_report(all_labels, all_preds, target_names=[\"Background\",\"Positive\"], digits=4))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(all_labels, all_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a45bddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def pick_best_frame(npz_path, batch_size=64):\n",
    "    case = np.load(npz_path, mmap_mode=\"r\")\n",
    "    images = case[\"image\"]   # (F,H,W)\n",
    "    labels = case[\"label\"]   # (F,)\n",
    "\n",
    "    best_idx, best_score = None, -1.0\n",
    "\n",
    "    for start in range(0, len(images), batch_size):\n",
    "        batch = images[start:start+batch_size].astype(np.float32)\n",
    "        b = torch.from_numpy(batch).unsqueeze(1)  # (B,1,H,W)\n",
    "\n",
    "        # normalize to [0,1]\n",
    "        b_min = b.amin(dim=(2,3), keepdim=True)\n",
    "        b_max = b.amax(dim=(2,3), keepdim=True)\n",
    "        b = (b - b_min) / (b_max - b_min + 1e-8)\n",
    "\n",
    "        # resize to match training (224,224)\n",
    "        b = F.interpolate(b, size=(224,224), mode=\"bilinear\", align_corners=False)\n",
    "        b = (b - 0.5) / 0.5   # normalization\n",
    "        b = b.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(b)\n",
    "            probs = torch.softmax(logits, dim=1)[:,1]  # positive prob\n",
    "\n",
    "        max_prob, max_idx = torch.max(probs, dim=0)\n",
    "        if max_prob.item() > best_score:\n",
    "            best_score = max_prob.item()\n",
    "            best_idx = start + max_idx.item()\n",
    "\n",
    "    # GT positives\n",
    "    positives = np.where(labels > 0)[0]\n",
    "    return best_idx, best_score, positives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b51842d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 54\u001b[0m\n\u001b[0;32m     51\u001b[0m val_loss, correct, total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m \u001b[43mval_loader\u001b[49m:\n\u001b[0;32m     55\u001b[0m         images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     56\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(images)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val_loader' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_case_level(val_files, root_dir, model, device, batch_size=64):\n",
    "    model.eval()\n",
    "    total_cases, case_hits = 0, 0\n",
    "\n",
    "    for fname in val_files:\n",
    "        case = np.load(os.path.join(root_dir, fname))\n",
    "        images = case[\"image\"].astype(np.float32)   # (F,H,W)\n",
    "        labels = case[\"label\"].astype(np.int64)     # (F,)\n",
    "        labels[labels == 2] = 1   # remap\n",
    "\n",
    "        positives = np.where(labels == 1)[0]\n",
    "\n",
    "        best_idx, best_score = None, -1.0\n",
    "        for start in range(0, len(images), batch_size):\n",
    "            batch = images[start:start+batch_size]\n",
    "            b = torch.from_numpy(batch).unsqueeze(1)  # (B,1,H,W)\n",
    "            b = (b - b.min()) / (b.max() - b.min() + 1e-8)  # normalize per-batch\n",
    "            b = F.interpolate(b, size=(224,224), mode=\"bilinear\", align_corners=False)\n",
    "            b = (b - 0.5) / 0.5  # [-1,1]\n",
    "            b = b.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(b)\n",
    "                probs = torch.softmax(logits, dim=1)[:,1]  # probability of positive\n",
    "\n",
    "            max_prob, max_idx = torch.max(probs, dim=0)\n",
    "            if max_prob.item() > best_score:\n",
    "                best_score = max_prob.item()\n",
    "                best_idx = start + max_idx.item()\n",
    "\n",
    "        # case success? (did we catch a positive if exists)\n",
    "        if len(positives) > 0:\n",
    "            success = best_idx in positives\n",
    "        else:\n",
    "            success = (labels[best_idx] == 0)\n",
    "\n",
    "        total_cases += 1\n",
    "        case_hits += int(success)\n",
    "\n",
    "    case_acc = 100 * case_hits / total_cases\n",
    "    print(f\"✅ Case-level Hit Rate: {case_hits}/{total_cases} ({case_acc:.2f}%)\")\n",
    "    return case_acc\n",
    "\n",
    "\n",
    "# === During Validation Loop ===\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "val_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        preds = outputs.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "val_loss /= len(val_loader)\n",
    "val_acc = 100 * correct / total\n",
    "\n",
    "print(\"\\n--- Frame-level Report ---\")\n",
    "print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "print(classification_report(all_labels, all_preds, digits=4))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(all_labels, all_preds))\n",
    "\n",
    "print(\"\\n--- Case-level Report ---\")\n",
    "val_files = sorted(os.listdir(\"D:/dataset/converted_classifier_npz_compact\"))[210:255]\n",
    "evaluate_case_level(val_files, \"D:/dataset/converted_classifier_npz_compact\", model, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f1cadab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, numpy as np, torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "random.seed(1337)\n",
    "\n",
    "# ---- Load model from checkpoint (dict) ----\n",
    "model = FrameClassifier(num_classes=2).to(device)\n",
    "ckpt = torch.load(\"D:/acouslic-ai-cse4622/saved_weights/best_frame_classifier.pth\",\n",
    "                  map_location=device)\n",
    "state = ckpt[\"model_state\"] if isinstance(ckpt, dict) and \"model_state\" in ckpt else ckpt\n",
    "model.load_state_dict(state)\n",
    "model.eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def _preprocess_batch_uint8_to_model(batch_uint8: np.ndarray, to224=True):\n",
    "    \"\"\"\n",
    "    batch_uint8: (B, H, W), dtype uint8 or float32 0..255\n",
    "    returns: (B,1,224,224) float32 normalized like training (mean=0.5,std=0.5)\n",
    "    \"\"\"\n",
    "    b = torch.from_numpy(batch_uint8).float()   # (B,H,W)\n",
    "    b = b.unsqueeze(1)                          # (B,1,H,W)\n",
    "    b = b / 255.0                               # [0,1]\n",
    "    if to224:\n",
    "        b = F.interpolate(b, size=(224,224), mode=\"bilinear\", align_corners=False)\n",
    "    # Normalize to match training: mean=0.5, std=0.5 -> [-1,1]\n",
    "    b = (b - 0.5) / 0.5\n",
    "    return b.to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def pick_best_frame(npz_path, batch_size=64):\n",
    "    case = np.load(npz_path, mmap_mode=\"r\")\n",
    "    images = case[\"image\"]  # (F,H,W), uint8\n",
    "    best_idx, best_score = None, -1.0\n",
    "\n",
    "    for start in range(0, len(images), batch_size):\n",
    "        batch = images[start:start+batch_size]          # uint8\n",
    "        b = _preprocess_batch_uint8_to_model(batch)     # (B,1,224,224)\n",
    "        logits = model(b)\n",
    "        probs = torch.softmax(logits, dim=1)[:, 1]      # P(positive)\n",
    "        max_prob, max_idx = torch.max(probs, dim=0)\n",
    "        if max_prob.item() > best_score:\n",
    "            best_score = max_prob.item()\n",
    "            best_idx = start + max_idx.item()\n",
    "    return best_idx, best_score\n",
    "\n",
    "def get_label_array(case_npz):\n",
    "    for key in [\"label\", \"labels\", \"y\", \"gt\", \"target\"]:\n",
    "        if key in case_npz:\n",
    "            arr = np.asarray(case_npz[key]).reshape(-1)\n",
    "            return arr\n",
    "    raise KeyError(\"No per-frame label key found (label/labels/y/gt/target).\")\n",
    "\n",
    "def evaluate_random_subset(root_dir, start_idx=0, k=12, tol=0):\n",
    "    \"\"\"\n",
    "    tol = tolerance window (in frames). tol=0 requires exact positive frame.\n",
    "    \"\"\"\n",
    "    files = sorted([f for f in os.listdir(root_dir) if f.endswith(\".npz\")])\n",
    "    test_files = files[start_idx:]\n",
    "    if not test_files:\n",
    "        raise RuntimeError(f\"No .npz files at/after index {start_idx} under {root_dir}\")\n",
    "    sample = random.sample(test_files, k=min(k, len(test_files)))\n",
    "\n",
    "    total = 0\n",
    "    hits_anypos = 0\n",
    "    exact_argmax_hits = 0\n",
    "    mean_prob_accum = []\n",
    "    dist_to_nearest_pos = []\n",
    "\n",
    "    print(f\"Evaluating {len(sample)} files (tol={tol})...\\n\")\n",
    "\n",
    "    for fname in sample:\n",
    "        path = os.path.join(root_dir, fname)\n",
    "        case = np.load(path, mmap_mode=\"r\")\n",
    "        labels = get_label_array(case)              # 0/1/2\n",
    "        positives = np.where((labels == 1) | (labels == 2))[0]\n",
    "\n",
    "        pred_idx, pred_prob = pick_best_frame(path)\n",
    "        total += 1\n",
    "        mean_prob_accum.append(pred_prob)\n",
    "\n",
    "        # Any positive within tolerance?\n",
    "        if positives.size > 0:\n",
    "            dists = np.abs(positives - pred_idx)\n",
    "            hit_tol = np.any(dists <= tol)\n",
    "            hits_anypos += int(hit_tol)\n",
    "            # \"argmax\" reference (first positive)\n",
    "            gt_argmax = positives[0]\n",
    "            exact_argmax_hits += int(abs(pred_idx - gt_argmax) <= tol)\n",
    "            dist = int(dists.min())\n",
    "        else:\n",
    "            # no positives in GT: success if prediction is background\n",
    "            hit_tol = (labels[pred_idx] == 0)\n",
    "            hits_anypos += int(hit_tol)\n",
    "            gt_argmax = int(np.argmax(labels))\n",
    "            exact_argmax_hits += int(pred_idx == gt_argmax)\n",
    "            dist = 0\n",
    "\n",
    "        dist_to_nearest_pos.append(dist)\n",
    "\n",
    "        print(f\"{fname}\")\n",
    "        print(f\"  Frames: {len(labels)} | Pred idx: {pred_idx:4d} (p={pred_prob:.4f})\")\n",
    "        print(f\"  GT positives: {positives.tolist()[:12]}{' ...' if len(positives)>12 else ''}\")\n",
    "        print(f\"  Hit (±{tol})? {'YES' if hit_tol else 'NO'} | Dist→nearest +ve: {dist}\\n\")\n",
    "\n",
    "    acc_anypos = hits_anypos / total\n",
    "    acc_exact  = exact_argmax_hits / total\n",
    "    mean_prob  = float(np.mean(mean_prob_accum))\n",
    "    mean_dist  = float(np.mean(dist_to_nearest_pos))\n",
    "\n",
    "    print(\"====== Summary ======\")\n",
    "    print(f\"Files evaluated:      {total}\")\n",
    "    print(f\"Hit any positive (±{tol}): {hits_anypos}/{total}  ({acc_anypos*100:.1f}%)\")\n",
    "    print(f\"Matches to first +ve (±{tol}): {exact_argmax_hits}/{total}  ({acc_exact*100:.1f}%)\")\n",
    "    print(f\"Mean predicted prob:  {mean_prob:.4f}\")\n",
    "    print(f\"Mean dist to +ve:     {mean_dist:.2f} frames\")\n",
    "    print(\"=====================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a36175e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 45 files (tol=0)...\n",
      "\n",
      "ea86047a-bae4-464b-a2ed-015935bebb2a.npz\n",
      "  Frames: 840 | Pred idx:   60 (p=0.9500)\n",
      "  GT positives: [50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61] ...\n",
      "  Hit (±0)? YES | Dist→nearest +ve: 0\n",
      "\n",
      "fc167d1b-045a-4057-936d-4862644af1f3.npz\n",
      "  Frames: 840 | Pred idx:   92 (p=0.9765)\n",
      "  GT positives: [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91] ...\n",
      "  Hit (±0)? YES | Dist→nearest +ve: 0\n",
      "\n",
      "f8039e25-4652-440c-9476-b425f3fccb22.npz\n",
      "  Frames: 840 | Pred idx:   56 (p=0.7737)\n",
      "  GT positives: [52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63] ...\n",
      "  Hit (±0)? YES | Dist→nearest +ve: 0\n",
      "\n",
      "d5c3cfee-53ac-4021-8c1b-098c189f630e.npz\n",
      "  Frames: 840 | Pred idx:  626 (p=0.9527)\n",
      "  GT positives: [20, 21, 22, 23, 24, 25, 164, 165, 166, 167, 168, 169] ...\n",
      "  Hit (±0)? YES | Dist→nearest +ve: 0\n",
      "\n",
      "d5471cfd-6090-4d42-9a95-67ccbfbf612e.npz\n",
      "  Frames: 840 | Pred idx:   42 (p=0.9714)\n",
      "  GT positives: [42, 43, 44, 45, 46, 47, 48, 176, 177, 178, 179, 180] ...\n",
      "  Hit (±0)? YES | Dist→nearest +ve: 0\n",
      "\n",
      "dcdcb9a4-fec7-45a2-b2b7-a282f963c551.npz\n",
      "  Frames: 840 | Pred idx:   59 (p=0.9482)\n",
      "  GT positives: [54, 55, 56, 57, 58, 59, 60, 61, 62, 63]\n",
      "  Hit (±0)? YES | Dist→nearest +ve: 0\n",
      "\n",
      "fc63df40-f6ca-4917-99ba-dbe4310a6d04.npz\n",
      "  Frames: 840 | Pred idx:   58 (p=0.9601)\n",
      "  GT positives: [605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616] ...\n",
      "  Hit (±0)? NO | Dist→nearest +ve: 547\n",
      "\n",
      "d77b6ece-da17-4f88-818c-0c7340b3e54f.npz\n",
      "  Frames: 840 | Pred idx:   58 (p=0.9902)\n",
      "  GT positives: [53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 217, 218] ...\n",
      "  Hit (±0)? YES | Dist→nearest +ve: 0\n",
      "\n",
      "e3670fa9-574c-4fbc-89af-f69aad63696d.npz\n",
      "  Frames: 840 | Pred idx:   52 (p=0.9782)\n",
      "  GT positives: [45, 46, 49, 50, 51, 52, 53, 312, 313, 314, 315, 316] ...\n",
      "  Hit (±0)? YES | Dist→nearest +ve: 0\n",
      "\n",
      "f5e97e20-ad47-46e4-bf35-3f0c01ff51d0.npz\n",
      "  Frames: 840 | Pred idx:   86 (p=0.8695)\n",
      "  GT positives: [81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91]\n",
      "  Hit (±0)? YES | Dist→nearest +ve: 0\n",
      "\n",
      "ec65aa35-d54c-44da-80a4-883f631c345e.npz\n",
      "  Frames: 840 | Pred idx:   32 (p=0.8984)\n",
      "  GT positives: [32, 33]\n",
      "  Hit (±0)? YES | Dist→nearest +ve: 0\n",
      "\n",
      "e5a2d914-a35c-4207-9bc8-5e5cee7dbfba.npz\n",
      "  Frames: 840 | Pred idx:   24 (p=0.9367)\n",
      "  GT positives: [158, 159, 160, 161, 162, 163, 164, 285, 286, 287, 288, 289] ...\n",
      "  Hit (±0)? NO | Dist→nearest +ve: 134\n",
      "\n",
      "d42fb920-5df1-4341-93df-480c17355e44.npz\n",
      "  Frames: 840 | Pred idx:  197 (p=0.9017)\n",
      "  GT positives: [799, 800, 801, 802, 803, 804, 805, 806]\n",
      "  Hit (±0)? NO | Dist→nearest +ve: 602\n",
      "\n",
      "e608e364-6898-442c-be0f-aea8d7735c15.npz\n",
      "  Frames: 840 | Pred idx:   49 (p=0.8979)\n",
      "  GT positives: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57]\n",
      "  Hit (±0)? YES | Dist→nearest +ve: 0\n",
      "\n",
      "de35b365-6308-4974-b2fc-976e26584810.npz\n",
      "  Frames: 840 | Pred idx:  652 (p=0.7105)\n",
      "  GT positives: [650, 651, 652, 653]\n",
      "  Hit (±0)? YES | Dist→nearest +ve: 0\n",
      "\n",
      "f5a702f0-d759-4b53-8215-462e8145e638.npz\n",
      "  Frames: 840 | Pred idx:  280 (p=0.8796)\n",
      "  GT positives: [30, 31, 32, 33, 34, 164, 165, 166, 167, 168, 169, 170] ...\n",
      "  Hit (±0)? NO | Dist→nearest +ve: 21\n",
      "\n",
      "ea724ff0-9990-4557-ab53-16f2fe1f1c8f.npz\n",
      "  Frames: 840 | Pred idx:  191 (p=0.9427)\n",
      "  GT positives: [51, 52, 53, 54, 55, 56, 57, 180, 181, 182, 183, 184] ...\n",
      "  Hit (±0)? NO | Dist→nearest +ve: 2\n",
      "\n",
      "d812091a-3635-4d51-9290-6adb3aa8681e.npz\n",
      "  Frames: 840 | Pred idx:   37 (p=0.9684)\n",
      "  GT positives: [34, 35, 36, 37, 38, 39, 179, 180, 181, 182, 183]\n",
      "  Hit (±0)? YES | Dist→nearest +ve: 0\n",
      "\n",
      "decd2d84-79a2-4239-b802-5afdf318e7e6.npz\n",
      "  Frames: 840 | Pred idx:  177 (p=0.8008)\n",
      "  GT positives: [43, 44, 45, 46, 47, 172, 173, 174, 175, 176, 177, 178] ...\n",
      "  Hit (±0)? YES | Dist→nearest +ve: 0\n",
      "\n",
      "f9ba7ffd-bc66-4da6-b97d-1286d760a184.npz\n",
      "  Frames: 840 | Pred idx:  616 (p=0.9933)\n",
      "  GT positives: [55, 56, 57, 58, 59, 324, 325, 326, 327, 328, 329, 330] ...\n",
      "  Hit (±0)? NO | Dist→nearest +ve: 274\n",
      "\n",
      "ea2a8fbf-f7fe-43f4-9e0d-1da0ae9b1a0c.npz\n",
      "  Frames: 840 | Pred idx:   30 (p=0.9576)\n",
      "  GT positives: [30, 31, 32, 33, 34, 35, 150, 151, 152, 153, 154, 155] ...\n",
      "  Hit (±0)? YES | Dist→nearest +ve: 0\n",
      "\n",
      "fe236600-a41c-4d26-b4ea-6d3ff109cf41.npz\n",
      "  Frames: 840 | Pred idx:  284 (p=0.9433)\n",
      "  GT positives: [282, 283, 284, 285, 286, 287, 288, 289, 290, 291]\n",
      "  Hit (±0)? YES | Dist→nearest +ve: 0\n",
      "\n",
      "f4488c3c-556e-43bd-95f9-ae4cbb7c7093.npz\n",
      "  Frames: 840 | Pred idx:   65 (p=0.8646)\n",
      "  GT positives: [510, 511, 512, 513, 514]\n",
      "  Hit (±0)? NO | Dist→nearest +ve: 445\n",
      "\n",
      "db9d468d-cb20-4d5e-b059-31728f5950e6.npz\n",
      "  Frames: 840 | Pred idx:  330 (p=0.9431)\n",
      "  GT positives: [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51] ...\n",
      "  Hit (±0)? NO | Dist→nearest +ve: 1\n",
      "\n",
      "f5e853b1-9465-40b0-a246-7d3f6cb94de7.npz\n",
      "  Frames: 840 | Pred idx:  474 (p=0.9408)\n",
      "  GT positives: [4, 5, 6, 7, 8, 9, 10, 44, 45, 46, 47, 157] ...\n",
      "  Hit (±0)? YES | Dist→nearest +ve: 0\n",
      "\n",
      "e5053a72-46f6-496d-8252-c8b531b5039b.npz\n",
      "  Frames: 840 | Pred idx:  209 (p=0.7611)\n",
      "  GT positives: [43, 44, 45, 46, 603, 604, 605]\n",
      "  Hit (±0)? NO | Dist→nearest +ve: 163\n",
      "\n",
      "d624338f-d09b-4bda-bbc3-3fa417015d6b.npz\n",
      "  Frames: 840 | Pred idx:  614 (p=0.9168)\n",
      "  GT positives: [64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75] ...\n",
      "  Hit (±0)? NO | Dist→nearest +ve: 266\n",
      "\n",
      "de964751-ec9d-4c53-9da3-9d7f200987b1.npz\n",
      "  Frames: 840 | Pred idx:   39 (p=0.9895)\n",
      "  GT positives: [28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39] ...\n",
      "  Hit (±0)? YES | Dist→nearest +ve: 0\n",
      "\n",
      "e9043ff5-f8a4-4095-bc44-26232e1be011.npz\n",
      "  Frames: 840 | Pred idx:   17 (p=0.9854)\n",
      "  GT positives: [13, 14, 15, 16, 17, 18, 140, 141, 142]\n",
      "  Hit (±0)? YES | Dist→nearest +ve: 0\n",
      "\n",
      "ee2e6220-b2f3-4c8e-9c85-f04ed0a6b6f7.npz\n",
      "  Frames: 840 | Pred idx:  483 (p=0.9068)\n",
      "  GT positives: [296, 297, 298, 299, 300, 301, 302, 481, 482, 483, 484, 485] ...\n",
      "  Hit (±0)? YES | Dist→nearest +ve: 0\n",
      "\n",
      "f5611c30-d04b-4691-9bd6-1e6de392a63c.npz\n",
      "  Frames: 840 | Pred idx:   72 (p=0.8226)\n",
      "  GT positives: [65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 219, 220] ...\n",
      "  Hit (±0)? YES | Dist→nearest +ve: 0\n",
      "\n",
      "dc0cbbdf-e4bb-4de5-958a-10576129e440.npz\n",
      "  Frames: 840 | Pred idx:   47 (p=0.9659)\n",
      "  GT positives: [42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53] ...\n",
      "  Hit (±0)? YES | Dist→nearest +ve: 0\n",
      "\n",
      "f256d725-9b06-4a88-a6b5-f9eee0f050f4.npz\n",
      "  Frames: 840 | Pred idx:   49 (p=0.9453)\n",
      "  GT positives: [41, 42, 43, 44, 45, 46, 47, 48, 49, 180, 181, 182] ...\n",
      "  Hit (±0)? YES | Dist→nearest +ve: 0\n",
      "\n",
      "dc381d50-124d-4bc8-b504-9c9c3a3ae697.npz\n",
      "  Frames: 840 | Pred idx:   47 (p=0.9895)\n",
      "  GT positives: [43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54] ...\n",
      "  Hit (±0)? YES | Dist→nearest +ve: 0\n",
      "\n",
      "ebadcfcb-a682-4493-8502-2adc194bba8c.npz\n",
      "  Frames: 840 | Pred idx:  471 (p=0.9756)\n",
      "  GT positives: [140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151] ...\n",
      "  Hit (±0)? YES | Dist→nearest +ve: 0\n",
      "\n",
      "f1fcabfc-f998-44c7-8420-c7a5ae5aaab7.npz\n",
      "  Frames: 840 | Pred idx:  624 (p=0.8398)\n",
      "  GT positives: [55]\n",
      "  Hit (±0)? NO | Dist→nearest +ve: 569\n",
      "\n",
      "eee8da44-8db7-4c9e-b52d-b6d461158a4e.npz\n",
      "  Frames: 840 | Pred idx:  610 (p=0.9349)\n",
      "  GT positives: [606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617] ...\n",
      "  Hit (±0)? YES | Dist→nearest +ve: 0\n",
      "\n",
      "de45de1a-2482-4024-ac8b-c18c4834b379.npz\n",
      "  Frames: 840 | Pred idx:  291 (p=0.7574)\n",
      "  GT positives: [46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56]\n",
      "  Hit (±0)? NO | Dist→nearest +ve: 235\n",
      "\n",
      "e9025780-a9bb-4300-be4e-fb220e824646.npz\n",
      "  Frames: 840 | Pred idx:   72 (p=0.8785)\n",
      "  GT positives: [70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 198] ...\n",
      "  Hit (±0)? YES | Dist→nearest +ve: 0\n",
      "\n",
      "d5f8c859-de93-4a50-b324-1ae4ad0267d4.npz\n",
      "  Frames: 840 | Pred idx:   64 (p=0.9390)\n",
      "  GT positives: [64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75] ...\n",
      "  Hit (±0)? YES | Dist→nearest +ve: 0\n",
      "\n",
      "fdfd752d-2d83-4452-a3ba-97a626676334.npz\n",
      "  Frames: 840 | Pred idx:   33 (p=0.8321)\n",
      "  GT positives: [31, 32, 33, 34, 35, 36, 224, 225, 226, 227, 228, 229] ...\n",
      "  Hit (±0)? YES | Dist→nearest +ve: 0\n",
      "\n",
      "d571d4e1-ff80-44b9-a481-07961c6a1208.npz\n",
      "  Frames: 840 | Pred idx:   48 (p=0.9138)\n",
      "  GT positives: [43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54] ...\n",
      "  Hit (±0)? YES | Dist→nearest +ve: 0\n",
      "\n",
      "de93dac8-41e0-4b03-946e-0accda6010af.npz\n",
      "  Frames: 840 | Pred idx:  525 (p=0.9149)\n",
      "  GT positives: [31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42] ...\n",
      "  Hit (±0)? NO | Dist→nearest +ve: 323\n",
      "\n",
      "f4bebd1e-d3ca-4186-902a-2b6ef9391060.npz\n",
      "  Frames: 840 | Pred idx:   41 (p=0.9620)\n",
      "  GT positives: [38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49] ...\n",
      "  Hit (±0)? YES | Dist→nearest +ve: 0\n",
      "\n",
      "d8c3665a-4dc3-40ce-b716-f30aab365332.npz\n",
      "  Frames: 840 | Pred idx:   40 (p=0.9110)\n",
      "  GT positives: [591, 592, 593, 594, 595, 596, 597]\n",
      "  Hit (±0)? NO | Dist→nearest +ve: 551\n",
      "\n",
      "====== Summary ======\n",
      "Files evaluated:      45\n",
      "Hit any positive (±0): 31/45  (68.9%)\n",
      "Matches to first +ve (±0): 4/45  (8.9%)\n",
      "Mean predicted prob:  0.9131\n",
      "Mean dist to +ve:     91.84 frames\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on full-length compacts (840 frames), e.g. last 30 files:\n",
    "evaluate_random_subset(\n",
    "    root_dir=\"D:/dataset/converted_classifier_npz_compact\",\n",
    "    start_idx=255,  # whatever split you used\n",
    "    k=45,\n",
    "    tol=0      # allow ±3 frames tolerance\n",
    ")\n",
    "\n",
    "# Or evaluate on your tiny 80-frame set (fast smoke test)\n",
    "\n",
    "evaluate_random_subset(\n",
    "    root_dir=\"D:/dataset/npz_80_tiny\",\n",
    "    start_idx=255,\n",
    "    k=45,\n",
    "    tol=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdba76f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
